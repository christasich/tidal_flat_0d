{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-dimensional model of sediment accumulation on Polder 32 in Southwest Bangladesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import feather\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing as mp\n",
    "import itertools\n",
    "import inspect\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add project root to path and import project specific definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from src.definitions import get_project_root\n",
    "\n",
    "root = get_project_root()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for the model run. Parameters must be single values (single core) or a list of values (parallel processing). If parallel flag is set to \"True\", the parallel method will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parallel = False\n",
    "# Additional parameters needed for parallelization\n",
    "if parallel is True:\n",
    "    poolsize = 30\n",
    "    chunksize = 1\n",
    "\n",
    "    # Quickly define regular interval for varying parameters.\n",
    "    # Can also be explicitly set.\n",
    "    slr = np.round(np.arange(0, 0.0325, 0.0025), 4)\n",
    "    ssc_factor = np.round(np.arange(0.25, 3.25, 0.25), 2)\n",
    "\n",
    "# Set model parameters\n",
    "run_length = 10  # years\n",
    "dt = '5 min'  # timestep must be given as a timedelta string\n",
    "slr = 0.003  # yearly rate (m) (0.002 ESLR + 0.001 Tidal Amp)\n",
    "\n",
    "ssc_factor = 1\n",
    "ssc = 0.2\n",
    "# ssc_file = './data/processed/ssc_by_week.csv'\n",
    "# ssc = pd.read_csv(ssc_file, index_col=0) * ssc_factor\n",
    "\n",
    "grain_dia = 0.000035  # grain diameter (m)\n",
    "grain_rho = 2650  # density of quartz\n",
    "bulk_rho = 1300  # g/L\n",
    "dP = 0\n",
    "dO = 0\n",
    "dM = 0\n",
    "A = 1\n",
    "z0 = 1.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to make tides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-5ac57c6146a9>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-5ac57c6146a9>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    if not os.path.isfile(file):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def make_combos(**kwargs):\n",
    "    \n",
    "    # Initialize arguments list and counter\n",
    "    var_args = []\n",
    "    const_args = {}\n",
    "    n = 0\n",
    "    \n",
    "    # Loop through all arguments and extract arguments with multiple values (i.e. arguments that will be varied)\n",
    "    for key, value in kwargs.items():\n",
    "        if isinstance(value, (list, tuple, np.ndarray)):\n",
    "            var_args.append(key)\n",
    "        else:\n",
    "            const_args.update({key : value})\n",
    "\n",
    "    # Initialize output with combination of all multi value arguments\n",
    "    combos = [dict(zip(var_args, i)) for i in itertools.product(*[eval(x) for x in var_args])]\n",
    "    \n",
    "    # Iterate through all combinations of dictionary key:value pairs and add single value arguments\n",
    "    for combo in combos:\n",
    "        for key, value in const_args.items():\n",
    "            combo.update({key : value})\n",
    "        combo.update({'n' : n}) # combination number for tracking in TQDM\n",
    "        n = n + 1\n",
    "\n",
    "    return combos\n",
    "\n",
    "# Creates a list of tides to be created. Useful when parallelizing and need to\n",
    "# create tides in advance.\n",
    "\n",
    "def list_missing_files(wdir, files):\n",
    "    missing_files = []\n",
    "    for file in files:\n",
    "        fp = wdir / arg\n",
    "        if fp.exists() is False:\n",
    "            missing_files.append(file)\n",
    "    \n",
    "\n",
    "def make_tide_list(slr, run_length, dt):\n",
    "    wdir = Path.cwd() / 'data' / 'interim' / 'tides'\n",
    "    if isinstance(slr, list):\n",
    "        pass\n",
    "    elif isinstance(slr_list, float):\n",
    "        slr_list = [slr_list]\n",
    "    tides_to_make = []\n",
    "    for rate in slr_list:\n",
    "        dt_min = int(pd.to_timedelta(dt).total_seconds() / 60)\n",
    "        fn = ('tides-yr_{0}-dt_{1}-slr_{2:.4f}.feather'\n",
    "              ).format(run_length, dt_min, rate)\n",
    "        fp = wdir / fn\n",
    "        if fp.exists() is False:\n",
    "            tides_to_make.append(rate)\n",
    "    return tides_to_make\n",
    "\n",
    "\n",
    "# Call Rscript and use OCE package to create idealized tide form to\n",
    "# be used in model\n",
    "\n",
    "\n",
    "def make_tides(params):\n",
    "    run_length = params.run_length\n",
    "    slr = params.slr\n",
    "    dt = params.dt\n",
    "    \n",
    "    R_command = 'Rscript'\n",
    "    wdir = Path.cwd()\n",
    "    script_path = wdir / 'scripts' / 'make_tides.R'\n",
    "    args = [str(run_length), str(dt), '{:.4f}'.format(slr), wdir]\n",
    "    cmd = [R_command, script_path] + args\n",
    "    # check_output will run the command and store to result\n",
    "    x = subprocess.check_output(cmd, universal_newlines=True)\n",
    "    dt_min = int(pd.to_timedelta(dt).total_seconds() / 60)\n",
    "    fn = ('tides-yr_{0}-dt_{1}-slr_{2:.4f}.feather'\n",
    "          ).format(run_length, dt_min, slr)\n",
    "    fp = wdir / 'data' / 'interim' / 'tides' / fn\n",
    "\n",
    "    tides = feather.read_dataframe(fp)\n",
    "    tides = tides.set_index('Datetime')\n",
    "\n",
    "    return tides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all tides before running the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-3-2016ee092e6e>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-2016ee092e6e>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(\"All tidal curves already constructed!')\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "tides_to_make = make_tide_list(slr, run_length, dt)\n",
    "\n",
    "if len(tides_to_make) == 0:\n",
    "    print(\"All tidal curves already constructed!\")\n",
    "elif len(tides_to_make) == 1:\n",
    "    make_tides(slr, run_length, dt)\n",
    "elif len(tides_to_make) >= 2:\n",
    "    print(\"Making {0} tidal curves.\".format(len(tides_to_make)))\n",
    "    \n",
    "    with mp.Pool(poolsize) as pool:\n",
    "        for new_tide in tqdm(pool.imap_unordered(make_tides,\n",
    "                                    tides_to_make), total=len(tides_to_make), unit='tidal curves'):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create main model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for running the model.\n",
    "# def run_model(tides, ssc, params, n = None)\n",
    "# Instead of grain_dia, you use params.grain_dia, params.grain_rho, etc.\n",
    "def run_model(tides, ssc, grain_dia, grain_rho, bulk_rho, dP=0, dO=0, dM=0, A=1, z0=0, n=None):\n",
    "    global num_runs\n",
    "    \n",
    "    # Function that sets the background SSC value given a method.\n",
    "    def find_ssc(ssc, method, timestamp=None):\n",
    "        if method == 'constant':\n",
    "            return ssc\n",
    "        elif method == 'weekly':\n",
    "            week = timestamp.week\n",
    "            ssc = ssc.loc[week].values[0]\n",
    "            return ssc\n",
    "\n",
    "    # Return suspended sediment values from a csv of average weekly suspended sediment for P32. When tide is below the\n",
    "    # the platform (no sedimentation) or the tide is falling (net export), \"0\" is returned.\n",
    "\n",
    "    # Calculate the concentration within the water column for a given timestep\n",
    "    # old method\n",
    "#     def calc_c(c0, h, h_min_1, dh, c_min_1, z, ws, dt):\n",
    "#         if (h > z and dh > 0):\n",
    "#             return (-c_min_1 * ws * dt + (c0-c_min_1) * (h - h_min_1)) / (h_min_1-z) + c_min_1\n",
    "#         elif (h > z and dh < 0):\n",
    "#             return (-c_min_1 * ws * dt) / (h_min_1-z) + c_min_1\n",
    "#         else:\n",
    "#             return 0\n",
    "        \n",
    "    # Method from work with David\n",
    "    def calc_c(c0, h, h_min_1, dh, c_min_1, z, ws, dt):\n",
    "        if (h > z and dh > 0):\n",
    "            return (-c_min_1 * ws * dt + (c0-c_min_1) * (h - h_min_1)) / (h_min_1-z) + c_min_1\n",
    "        elif (h > z and dh < 0):\n",
    "            return (-c_min_1 * ws * dt) / (h_min_1-z) + c_min_1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "#     added by JG\n",
    "#     global g_calc_c\n",
    "#     g_calc_c = calc_c\n",
    "\n",
    "    # Calculate the change in elevation for a given timestep    \n",
    "    def calc_dz(c, ws, rho, dt):\n",
    "        return ws * c * dt / rho\n",
    "\n",
    "    # Add the change of elevation back to the original elevation\n",
    "    def calc_z(z_min_1, dz_min_1, dO, dP, dM):\n",
    "        return z_min_1 + dz_min_1 + dO - dP - dM\n",
    "    \n",
    "    # Set method to be used for ssc based on ssc input type\n",
    "    if isinstance(ssc, float):\n",
    "        ssc_method = 'constant'\n",
    "    elif isinstance(ssc, pd.DataFrame):\n",
    "        ssc_method = 'weekly'\n",
    "    \n",
    "    # Set Datetime as the index. Feather does not export non-integer indices.\n",
    "    tides = tides.set_index('Datetime')\n",
    "    index = tides.index\n",
    "    dt = index[1] - index[0]\n",
    "    dt_sec = dt.total_seconds()\n",
    "    \n",
    "    # Convert constant rates from yearly to dt\n",
    "    dO = dO / 8760 / 60 / 60 * dt_sec\n",
    "    dP = dP / 8760 / 60 / 60 * dt_sec\n",
    "    dM = dM / 8760 / 60 / 60 * dt_sec\n",
    "    \n",
    "    # Assume density and viscosity of water\n",
    "    fluid_rho = 1000\n",
    "    fluid_visc = 0.001\n",
    "    g = 9.8\n",
    "    \n",
    "    # Calculate settling velocity using Stokes settling. Considered an upper bound for possible settling velocity.\n",
    "    ws = (2/9 * (grain_rho - fluid_rho) / fluid_visc) * g * (grain_dia / 2) ** 2\n",
    "    \n",
    "    # Initialize numpy arrays for efficiency\n",
    "    z = np.zeros(len(tides.index))\n",
    "    z[0] = z0\n",
    "    h = tides.pressure.values\n",
    "    dh = np.insert(np.diff(h) / dt_sec,0,np.nan)\n",
    "    inundated = np.zeros(len(tides.index))\n",
    "    inundation_depth = np.zeros(len(tides.index))\n",
    "    C0 = np.zeros(len(tides.index))\n",
    "    C = np.zeros(len(tides.index))\n",
    "    dz = np.zeros(len(tides.index))\n",
    "    SSC = np.zeros(len(tides.index))\n",
    "    \n",
    "#     def update_elevation(z, SSC, C0, C, dz, h, dh, dO, dP, dM, A, ssc, bulk_rho, dt_sec, ws, ssc_method, timestamp):\n",
    "#         z[t] = calc_z(z[t-1], dz[t-1], dO, dP, dM)\n",
    "#         SSC[t] = find_ssc(ssc, method=ssc_method, timestamp=index[t])\n",
    "#         C0[t] = calc_c0(h[t], dh[t], ssc, z[t], A)\n",
    "#         C[t] = calc_c(C0[t], h[t], h[t-1], dh[t], C[t-1], z[t], ws, dt_sec)\n",
    "#         dz[t] = calc_dz(C[t], ws, bulk_rho, dt_sec)\n",
    "    \n",
    "    # For loop to calculate backwards difference approximation.\n",
    "    # TQDM is a wrapper that shows a status bar while calculating.\n",
    "    counter = np.arange(1,len(index))\n",
    "    for t in tqdm(counter, \n",
    "                  desc='Run {0} of {1} [PID: {2}]'.format(n, num_runs, os.getpid()), \n",
    "                  total=len(index[1:]), \n",
    "                  unit='steps'):\n",
    "        # update_elevation(z = z, SSC = SSC, C0 = C0, C = C, \n",
    "        #                  dz = dz, h =  h, dh = dh, dO = dO, dP = dP, dM = dM, \n",
    "        #                  A = A, ssc = ssc, bulk_rho = bulk_rho, dt_sec = dt_sec, \n",
    "        #                  ws = ws, ssc_method = ssc_method, timestamp = timestamp)\n",
    "        z[t] = calc_z(z[t-1], dz[t-1], dO, dP, dM)\n",
    "        SSC[t] = find_ssc(ssc, method=ssc_method, timestamp=index[t])\n",
    "        C0[t] = SSC[t] * A\n",
    "        C[t] = calc_c(C0[t], h[t], h[t-1], dh[t], C[t-1], z[t], ws, dt_sec)\n",
    "        dz[t] = calc_dz(C[t], ws, bulk_rho, dt_sec)\n",
    "        # Flag if inundated and by how much\n",
    "        if h[t] - z[t] >= 0:\n",
    "            inundated[t] = 1\n",
    "            inundation_depth[t] = h[t] - z[t]\n",
    "    \n",
    "    # Create pandas dataframe from numpy arrays of finite difference results\n",
    "    d = {'h' : h, 'dh' : dh, 'C0' : C0, 'C' : C, 'dz' : dz, 'z' : z, \n",
    "         'inundated' : inundated, 'inundation_depth' : inundation_depth}\n",
    "    df = pd.DataFrame(data=d, index = tides.index)\n",
    "    \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates combination of parameter values for different scenarios\n",
    "def make_combos(run_length, dt, slr, ssc_factor, grain_diameter, grain_rho, bulk_rho, dP, dO, dM, A, z0):\n",
    "    \n",
    "    # Use inspect function to get all of the input arguments\n",
    "    args = inspect.getfullargspec(make_combos).args\n",
    "    \n",
    "    # Initialize arguments list and counter\n",
    "    multi_args = []\n",
    "    n = 0\n",
    "    \n",
    "    # Loop through all arguments and extract arguments with multiple values (i.e. arguments that will be varied)\n",
    "    for arg in args:\n",
    "        if isinstance(eval(arg), (list, tuple, np.ndarray)):\n",
    "            multi_args.append(arg)\n",
    "            \n",
    "    # Extract the arguments with one value (i.e. arguments that will remain constant)\n",
    "    single_args = list(set(args) - set(multi_args))\n",
    "    \n",
    "    # Create a dictionary of single value arguments\n",
    "    single_dict = [{'{0}'.format(j) : eval(j)} for j in single_args]\n",
    "    \n",
    "    # Initialize output with combination of all multi value arguments\n",
    "    combos = [dict(zip(multi_args, i)) for i in itertools.product(*[eval(x) for x in multi_args])]\n",
    "    \n",
    "    # Iterate through all combinations of dictionary key:value pairs and add single value arguments\n",
    "    for combo in combos:\n",
    "        for item in single_dict:\n",
    "            combo.update(item)\n",
    "        combo.update({'n' : n}) # combination number for tracking in TQDM\n",
    "        n = n + 1\n",
    "\n",
    "    return combos\n",
    "\n",
    "# Function to be called by parallel function (e.g. imap_unordered). This is necessary because imap only accepts\n",
    "# one function and one iterable. Using this parser allows the model to be run with multiple iterables package as \n",
    "# one tuple\n",
    "def parallel_parser(in_data):\n",
    "\n",
    "    n = in_data['n'] # number to explicitly set print line for TQDM. Not working.\n",
    "    \n",
    "    # Load tides for a given run_length, dt, and slr from the tide library.\n",
    "    run_length = in_data['run_length']\n",
    "    dt = in_data['dt']\n",
    "    slr = in_data['slr']\n",
    "    tides = feather.read_dataframe(\n",
    "        './data/interim/tides/tides-yr_{0}-dt_{1}-slr_{2}.feather'.format(run_length, \n",
    "                                                                          int(pd.to_timedelta(dt).total_seconds()/60/60), \n",
    "                                                                          '%.4f' % slr))\n",
    "    # Set Datetime as the index. Feather does not export non-integer indices.\n",
    "    tides = tides.set_index('Datetime')\n",
    "    \n",
    "    # Load weekly ssc data. Original data from OBS sensor deployed at Sutarkhali. Data is in 1-min increments. Developed\n",
    "    # a model to predict SSC by week of the year (incoming_ssc.R). Output of this script is the weekly SSC loaded here.\n",
    "    ssc_factor = in_data['ssc_factor'] # scaling factor used to adjust SSC\n",
    "    ssc_file = './data/processed/ssc_by_week.csv'\n",
    "    ssc = pd.read_csv(ssc_file, index_col=0) * ssc_factor\n",
    "    \n",
    "    # set parameters for model run\n",
    "    grain_dia = in_data['grain_dia']\n",
    "    grain_rho = in_data['grain_rho']\n",
    "    bulk_rho = in_data['bulk_rho']\n",
    "    dP = in_data['dP']\n",
    "    dO = in_data['dO']\n",
    "    dM = in_data['dM']\n",
    "    A = in_data['A']\n",
    "    z0 = in_data['z0']\n",
    "    \n",
    "    # run model\n",
    "    # params = in_data[\"params\"]\n",
    "    # run_model(tides = tides, ssc = ssc, params = params, n = n)\n",
    "    run_model(tides, ssc, grain_dia, grain_rho, bulk_rho, dP, dO, dM, A, z0, n=n)\n",
    "    \n",
    "    # Write results to a feather file\n",
    "    out_name = 'yr_{0}-slr_{1}-grain_dia_{2}-grain_rho_{3}-bulk_rho_{4}-sscfactor_{5}-dP_{6}-dM_{7}-A_{8}-z0_{9}.feather'.format(run_length, slr, grain_dia, grain_rho, bulk_rho, ssc_factor, dP, dM, A, z0)\n",
    "    feather.write_dataframe(df.reset_index(), './data/interim/results/{0}'.format(out_name))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d228502c67e74d71939582429fc0b7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Run 1 of 1 [PID: 1575]', max=1052064.0, style=ProgressStyâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parallel method\n",
    "if parallel == True:\n",
    "    \n",
    "    # Make combos of parameters\n",
    "    model_runs = make_combos(run_length, dt, slr, ssc_factor, grain_dia, grain_rho, bulk_rho, dP, dO, dM, A, z0)\n",
    "    \n",
    "    # Count number of models to be run\n",
    "    num_runs = len(model_runs)\n",
    "\n",
    "    # Initialize pool and run models\n",
    "    with mp.Pool(poolsize) as pool:\n",
    "        for result in pool.imap_unordered(parallel_parser, model_runs, chunksize=chunksize):\n",
    "            pass\n",
    "\n",
    "# Single core method\n",
    "elif parallel == False:\n",
    "    \n",
    "    # Load tides from tide library\n",
    "    tides = feather.read_dataframe('./data/interim/tides/tides-yr_{0}-dt_{1}-slr_{2}.feather'.format(\n",
    "        run_length, int(pd.to_timedelta(dt).total_seconds()/60), '%.4f' % slr))\n",
    "\n",
    "    # Set number of models to be run\n",
    "    num_runs = 1\n",
    "\n",
    "    # run model\n",
    "    # df = run_model(tides = tides, ssc = ssc, params = params, n = 1)\n",
    "    df = run_model(tides, ssc, grain_dia, grain_rho, bulk_rho, dP, dO, dM, A, z0, n=1)\n",
    "    \n",
    "    # write results to feather file\n",
    "    out_name = 'yr_{0}-slr_{1}-grain_dia_{2}-grain_rho_{3}-bulk_rho_{4}-sscfactor_{5}-dP_{6}-dM_{7}-A_{8}-z0_{9}.feather'.format(run_length, slr, grain_dia, grain_rho, bulk_rho, ssc_factor, dP, dM, A, z0)\n",
    "    feather.write_dataframe(df.reset_index(), './data/interim/results/{0}'.format(out_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr1 = np.sum(df['2014-05-17':'2015-05-17'].dz)*100\n",
    "yr2 = np.sum(df['2015-05-17':'2016-05-17'].dz)*100\n",
    "yr3 = np.sum(df['2016-05-17':'2017-05-17'].dz)*100\n",
    "yr4 = np.sum(df['2017-05-17':'2018-05-17'].dz)*100\n",
    "yr5 = np.sum(df['2018-05-17':'2019-05-17'].dz)*100\n",
    "yr6 = np.sum(df['2019-05-17':'2020-05-17'].dz)*100\n",
    "yr10 = np.sum(df['2023-05-17':'2024-05-17'].dz)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-17536132.25389067 -16276255.494442893 -14466147.63276091 -13377189.73421206 -12588479.468255972 -12003088.121481061 -10362995.486001689\n"
     ]
    }
   ],
   "source": [
    "print(yr1, yr2, yr3, yr4, yr5, yr6, yr10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot object at 0x7f915be53850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.plot(y = ['h','z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "yr1 = np.sum(df['2014-05-17':'2015-05-17'].dz)*100\n",
    "yr2 = np.sum(df['2015-05-17':'2016-05-17'].dz)*100\n",
    "yr3 = np.sum(df['2016-05-17':'2017-05-17'].dz)*100\n",
    "yr4 = np.sum(df['2017-05-17':'2018-05-17'].dz)*100\n",
    "yr5 = np.sum(df['2018-05-17':'2019-05-17'].dz)*100\n",
    "yr6 = np.sum(df['2019-05-17':'2020-05-17'].dz)*100\n",
    "yr7 = np.sum(df['2020-05-17':'2021-05-17'].dz)*100\n",
    "yr8 = np.sum(df['2021-05-17':'2022-05-17'].dz)*100\n",
    "yr9 = np.sum(df['2022-05-17':'2023-05-17'].dz)*100\n",
    "yr10 = np.sum(df['2023-05-17':'2024-05-17'].dz)*100\n",
    "dfz = pd.DataFrame((yr1, yr2, yr3, yr4, yr5, yr6, yr7, yr8, yr9, yr10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = feather.read_dataframe('data/interim/tides/tides-yr_1-dt_5-slr_0.0020.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('tides.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('..')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
