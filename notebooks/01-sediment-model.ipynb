{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-dimensional model of sediment accumulation on Polder 32 in Southwest Bangladesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chris/projects/tidal_flat_0d/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools as it\n",
    "import inspect\n",
    "import shutil\n",
    "import re\n",
    "from collections import namedtuple\n",
    "from pyprojroot import here\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set project root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = here()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for the model run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cpu_num = 1\n",
    "\n",
    "# Set model parameters\n",
    "run_len = 1  # years\n",
    "dt = \"1 sec\"  # timestep must be given as a timedelta string\n",
    "slr = 0.005  # yearly rate (m) (0.002 ESLR + 0.001 Tidal Amp)\n",
    "\n",
    "if cpu_num != 1:\n",
    "    slr = np.round(np.arange(0, 0.0325, 0.0025), 4)\n",
    "    ssc_factor = np.round(np.arange(0.25, 3.25, 0.25), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make tides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions that will be used to search for any missing tides and then create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_combos(**kwargs):\n",
    "    '''\n",
    "    Function that takes n-kwargs and returns a list of namedtuples\n",
    "    for each possible combination of kwargs.\n",
    "    '''\n",
    "    for key, value in kwargs.items():\n",
    "        if isinstance(value, (list, tuple, np.ndarray)) is False:\n",
    "            kwargs.update({key: [value]})\n",
    "    keys, value_tuples = zip(*kwargs.items())\n",
    "    combo_tuple = namedtuple(\"combos\", keys)\n",
    "    combos = [combo_tuple(*values) for values in it.product(*value_tuples)]\n",
    "    return combos\n",
    "\n",
    "\n",
    "def construct_filename(fn_format, **kwargs):\n",
    "    '''\n",
    "    Function that takes a string with n-number of format placeholders (e.g. {0]})\n",
    "    and uses the values from n-kwargs to populate the string.\n",
    "    '''\n",
    "    kwarg_num = len(kwargs)\n",
    "    fn_var_num = len(re.findall(r\"\\{.*?\\}\", fn_format))\n",
    "    if kwarg_num != fn_var_num:\n",
    "        raise Exception(\n",
    "            \"Format error: Given {0} kwargs, but \"\n",
    "            \"filename format has {1} sets of \"\n",
    "            \"braces.\".format(kwarg_num, fn_var_num)\n",
    "        )\n",
    "    fn = fn_format.format(*kwargs.values())\n",
    "    return fn\n",
    "\n",
    "\n",
    "def search_file(wdir, filename):\n",
    "    '''\n",
    "    Function that searches a directory for a filename and returns 0 the number\n",
    "    of exact matches (0 or 1). If more than one file is found, the function\n",
    "    will raise an exception.\n",
    "    '''\n",
    "    if len(list(Path(wdir).glob(filename))) == 0:\n",
    "        found = 0\n",
    "    elif len(list(Path(wdir).glob(filename))) == 1:\n",
    "        found = 1\n",
    "    elif len(list(Path(wdir).glob(filename))) > 1:\n",
    "        raise Exception(\"Found too many files that match.\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def missing_combos(wdir, fn_format, combos):\n",
    "    '''\n",
    "    Function that creates filenames for a list of combinations and \n",
    "    then searches a directory for the filenames. The function returns\n",
    "    a list of combinations that were not found.\n",
    "    '''\n",
    "    to_make = []\n",
    "    for combo in combos:\n",
    "        fn = construct_filename(\n",
    "            fn_format=fn_format,\n",
    "            run_len=combo.run_len,\n",
    "            dt=int(pd.to_timedelta(combo.dt).total_seconds()),\n",
    "            slr=combo.slr,\n",
    "        )\n",
    "        if search_file(wdir, fn) == 0:\n",
    "            to_make.append(combo)\n",
    "    return to_make\n",
    "\n",
    "\n",
    "def make_tide(params):\n",
    "    '''\n",
    "    Function that accepts a namedtuple or dictionary object containing\n",
    "    the arguments: wdir, fn_format, run_length, dt, and slr. These values\n",
    "    are passed to the Rscript make_tides.R which creates a discretized tidal\n",
    "    curve with timesteps of dt and a total length of run_len. Sea level rise\n",
    "    is added to the curve using a yearly rate of SLR. The tidal data is stored in\n",
    "    wdir as a feather file for interopability between R and Python.\n",
    "    '''\n",
    "    fn = construct_filename(\n",
    "        fn_format=params.fn_format,\n",
    "        run_len=params.run_len,\n",
    "        dt=int(pd.to_timedelta(params.dt).total_seconds()),\n",
    "        slr=params.slr,\n",
    "    )\n",
    "    if params.wdir.is_dir() is False:\n",
    "        params.wdir.mkdir()\n",
    "\n",
    "    R_command = \"Rscript\"\n",
    "    script_path = (root / \"scripts\" / \"make_tides.R\").absolute().as_posix()\n",
    "    args = [\n",
    "        str(params.run_len),\n",
    "        str(params.dt),\n",
    "        \"{:.4f}\".format(params.slr),\n",
    "        params.wdir.absolute().as_posix(),\n",
    "    ]\n",
    "    cmd = [R_command, script_path] + args\n",
    "    subprocess.check_output(cmd, universal_newlines=True)\n",
    "    msg = \"Tide created: {0}\".format(fn)\n",
    "    return msg\n",
    "\n",
    "\n",
    "def load_tide(wdir, filename):\n",
    "    '''\n",
    "    Function that loads the tidal curve constructed by make_tides.R\n",
    "    and sets the index to the Datetime column.\n",
    "    '''\n",
    "    fp = wdir / filename\n",
    "    tides = feather.read_dataframe(fp)\n",
    "    tides = tides.set_index(\"Datetime\")\n",
    "\n",
    "    return tides\n",
    "\n",
    "\n",
    "def continuous_to_discrete_tides(fx, x=None, periods=None):\n",
    "    if x is None and period is None:\n",
    "        raise Exception(\"x or periods must be specified.\")\n",
    "    elif x is not None and periods is not none:\n",
    "        raise Exception(\"Only x or periods can be specified; not both.\")\n",
    "    elif isinstance(x, (list, tuple, np.ndarray, pd.RangeIndex, pd.DatetimeIndex)) is False:\n",
    "        raise Exception(\"x must be a list, tuple, numpy array, or pandas range index.\")\n",
    "    elif isinstance(periods, (int, float)) is False:\n",
    "        raise Exception('periods must be an int or a float.')\n",
    "    fun = lambda x: eval(fx)\n",
    "    data = [fun(i) for i in np.arange(len(x))]\n",
    "    df = pd.DataFrame(data = data, columns = ['pressure'], index=x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = root / \"data\" / \"interim\"\n",
    "fpath = wdir / \"sutarkhali_pressure.csv\"\n",
    "df = pd.read_csv(fpath, index_col = 'Datetime', parse_dates=True)\n",
    "df = df.dropna()\n",
    "df = df.assign(Pressure = df.Pressure - np.mean(df.Pressure))\n",
    "r_df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = wdir / 'out.feather'\n",
    "feather.write_dataframe(r_df, path, compression='uncompressed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_command = \"Rscript\"\n",
    "script_path = (root / \"scripts\" / \"model_tides.R\").absolute().as_posix()\n",
    "args = [\n",
    "    df_feather_path.absolute().as_posix(),\n",
    "    dt,\n",
    "    (wdir / 'tides.feather').absolute().as_posix(),\n",
    "]\n",
    "cmd = [R_command, script_path] + args\n",
    "subprocess.check_output(cmd, universal_newlines=True)\n",
    "msg = \"Tide created: {0}\".format(fn)\n",
    "return msg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all missing tides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = root / \"data\" / \"interim\" / \"tides\"\n",
    "fn_format = \"tides-yr_{0}-dt_{1}-slr_{2:.4f}.feather\"\n",
    "tide_combos = make_combos(slr=slr, run_len=run_len, dt=dt)\n",
    "missing = missing_combos(wdir, fn_format, tide_combos)\n",
    "params_for_make_tide = namedtuple(\n",
    "    \"params_for_make_tide\", (\"wdir\", \"fn_format\") + tide_combos[0]._fields\n",
    ")\n",
    "tides_to_make = [\n",
    "    params_for_make_tide(\n",
    "        wdir=wdir, fn_format=fn_format, slr=item.slr, run_len=item.run_len, dt=item.dt\n",
    "    )\n",
    "    for item in missing\n",
    "]\n",
    "\n",
    "if len(tides_to_make) > 0:\n",
    "    Parallel(n_jobs=cpu_num)(delayed(make_tide)(i) for i in tides_to_make)\n",
    "else:\n",
    "    print('All tides already in library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = root / \"data\" / \"interim\" / \"tides\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the sediment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CT METHOD\n",
    "# def calc_conc(\n",
    "#     bound_conc, tide_height, tide_height_min_1, conc_min_1, elev, elev_min_1, settle_rate, timestep\n",
    "# ):\n",
    "#     depth = tide_height - elev\n",
    "#     depth_min_1 = tide_height_min_1 - elev_min_1\n",
    "#     change_in_depth = depth - depth_min_1\n",
    "\n",
    "#     # Checks\n",
    "#     tide_above_platform = depth > 0\n",
    "#     depth_stable = depth_min_1 >= 0.0015\n",
    "#     tide_increasing = tide_height > tide_height_min_1\n",
    "#     settling_valid = settle_rate * conc_min_1 * timestep <= depth_min_1 * conc_min_1\n",
    "\n",
    "#     term1 = conc_min_1\n",
    "#     term2 = (settle_rate * conc_min_1) / depth_min_1 * timestep\n",
    "#     term3 = 1 / depth_min_1\n",
    "#     term4 = bound_conc * change_in_depth\n",
    "#     term5 = conc_min_1 * change_in_depth\n",
    "\n",
    "#     if not tide_increasing:\n",
    "#         term3 = 0\n",
    "#     if not settling_valid:\n",
    "#         term1 = 0\n",
    "#         term2 = 0\n",
    "#     if tide_above_platform and depth_stable:\n",
    "#         conc = term1 - term2 + term3 * (term4 - term5)\n",
    "#     else:\n",
    "#         conc = 0\n",
    "\n",
    "#     return conc\n",
    "\n",
    "#JG Method\n",
    "def calc_conc(\n",
    "    bound_conc, tide_height, tide_height_min_1, conc_min_1, elev, elev_min_1, settle_rate, timestep\n",
    "):\n",
    "    depth = tide_height - elev\n",
    "    depth_min_1 = tide_height_min_1 - elev_min_1\n",
    "    change_in_depth = depth - depth_min_1\n",
    "    \n",
    "    # Checks\n",
    "    tide_above_platform = np.bool(tide_height > elev)\n",
    "    prev_tide_above_platform = np.bool(tide_height_min_1 > elev)\n",
    "    tide_increasing = np.bool(tide_height > tide_height_min_1)\n",
    "    settling_valid = np.bool(settle_rate * timestep < depth_min_1)\n",
    "    checks = [tide_above_platform, tide_increasing, settling_valid]\n",
    "    if not prev_tide_above_platform:\n",
    "        depth_min_1 = 0\n",
    "        change_in_depth = depth\n",
    "        settling_valid = False\n",
    "    term1 = conc_min_1\n",
    "    term2 = depth_min_1 - settle_rate * timestep\n",
    "    term3 = 1\n",
    "    term4 = bound_conc * change_in_depth\n",
    "    term5 = depth\n",
    "    if not tide_increasing:\n",
    "        term3 = 0\n",
    "    if not prev_tide_above_platform:\n",
    "        term1 = 0\n",
    "        term2 = 0\n",
    "        term4 = term4 - bound_conc * settle_rate * timestep\n",
    "        if term4 < 0:\n",
    "            term4 = 0\n",
    "    if not settling_valid:\n",
    "        term2 = 0\n",
    "    if tide_above_platform:\n",
    "        conc = (term1 * term2 + term3 * term4) / term5\n",
    "    else:\n",
    "        conc = 0\n",
    "\n",
    "    return conc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions that will be used to run the sediment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_bound_conc(bound_conc, method, timestamp=None):\n",
    "    if method == \"constant\":\n",
    "        return bound_conc\n",
    "    elif method == \"weekly\":\n",
    "        week = timestamp.week\n",
    "        bound_conc = sed_conc.loc[week].values[0]\n",
    "        return bound_conc\n",
    "\n",
    "\n",
    "def calc_conc(\n",
    "    bound_conc, tide_height, prev_tide_height, prev_conc, elev, prev_elev, settle_rate, timestep\n",
    "):\n",
    "    depth = tide_height - elev\n",
    "    prev_depth = prev_tide_height - prev_elev\n",
    "    change_in_depth = depth - prev_depth\n",
    "\n",
    "    # Checks\n",
    "    tide_above_platform = depth > 0\n",
    "    depth_stable = prev_depth >= 0.0015\n",
    "    tide_increasing = tide_height > prev_tide_height\n",
    "    settling_valid = settle_rate * prev_conc * timestep <= prev_depth * prev_conc\n",
    "\n",
    "    if tide_above_platform and depth_stable:\n",
    "        if tide_increasing:\n",
    "            conc = prev_conc - (settle_rate * prev_conc) / prev_depth * timestep + 1 / prev_depth * (bound_conc * change_in_depth - prev_conc * change_in_depth)\n",
    "            return conc\n",
    "        if not tide_increasing:\n",
    "            conc = prev_conc - (settle_rate * prev_conc) / prev_depth * timestep\n",
    "            return conc\n",
    "        else:\n",
    "            raise Exception('Tide not increasing or decreasing.')\n",
    "    if not tide_above_platform or not depth_stable:\n",
    "        conc = 0\n",
    "        return conc\n",
    "    else:\n",
    "        raise Exception('Error in tide_above_platform or depth_stable')\n",
    "    return conc\n",
    "\n",
    "\n",
    "def accumulate_sediment(conc, settle_rate, timestep):\n",
    "    deposited_sediment = settle_rate * conc * timestep\n",
    "    return deposited_sediment\n",
    "\n",
    "\n",
    "def aggrade(start_elev, sediment, organic, compaction, subsidence):\n",
    "    elev = start_elev + sediment + organic - compaction - subsidence\n",
    "    return elev\n",
    "\n",
    "\n",
    "def return_settle_rate(grain_den, grain_dia):\n",
    "    fluid_den = 1000\n",
    "    fluid_visc = 0.001\n",
    "    g = 9.8\n",
    "    settle_rate = (\n",
    "        (2 / 9 * (grain_den - fluid_den) / fluid_visc) * g * (grain_dia / 2) ** 2\n",
    "    )\n",
    "    return settle_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_conc(\n",
    "    bound_conc, tide_height, prev_tide_height, prev_conc, elev, prev_elev, settle_rate, timestep\n",
    "):\n",
    "    depth = tide_height - elev\n",
    "    prev_depth = prev_tide_height - prev_elev\n",
    "    change_in_depth = depth - prev_depth\n",
    "\n",
    "    # Checks\n",
    "    tide_above_platform = depth > 0\n",
    "    depth_stable = prev_depth >= 0.0015\n",
    "    tide_increasing = tide_height > prev_tide_height\n",
    "    settling_valid = settle_rate * prev_conc * timestep <= prev_depth * prev_conc\n",
    "\n",
    "    if tide_above_platform and depth_stable:\n",
    "        if tide_increasing:\n",
    "            conc = prev_conc - (settle_rate * prev_conc) / prev_depth * timestep + 1 / prev_depth * (bound_conc * change_in_depth - prev_conc * change_in_depth)\n",
    "            return conc\n",
    "        if not tide_increasing:\n",
    "            conc = prev_conc - (settle_rate * prev_conc) / prev_depth * timestep\n",
    "            return conc\n",
    "        else:\n",
    "            raise Exception('Tide not increasing or decreasing.')\n",
    "    if not tide_above_platform or not depth_stable:\n",
    "        conc = 0\n",
    "        return conc\n",
    "    else:\n",
    "        raise Exception('Error in tide_above_platform or depth_stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model. Work in progress. Normally, these parameters will be defined at the top of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_conc = 0.8 # g/L\n",
    "grain_dia = 0.000035  # grain diameter (m)\n",
    "grain_den = 2650  # density of quartz kg/m3\n",
    "bulk_den = 1100 # kg/m3\n",
    "compaction_rate = 0\n",
    "organic_rate = 0\n",
    "subsidence_rate = 0\n",
    "start_elev = 1.25 - 0.45\n",
    "tidal_amplification = 1\n",
    "timestep = '1s'\n",
    "#tide_name = \"tides-yr_1-dt_1-slr_0.0050.feather\"\n",
    "tide_name = \"tides-debug.feather\"\n",
    "\n",
    "tides = load_tide(wdir, tide_name).resample(timestep).asfreq() * tidal_amplification\n",
    "index = tides.index\n",
    "timestep = index[1] - index[0]\n",
    "timestep_sec = timestep.total_seconds()\n",
    "\n",
    "organic = organic_rate / 8760 / 60 / 60 * timestep_sec\n",
    "compaction = compaction_rate / 8760 / 60 / 60 * timestep_sec\n",
    "subsidence = subsidence_rate / 8760 / 60 / 60 * timestep_sec\n",
    "settle_rate = return_settle_rate(grain_den, grain_dia)\n",
    "\n",
    "elev = np.zeros(len(tides.index))\n",
    "elev[0] = start_elev\n",
    "elev_change = np.zeros(len(tides.index))\n",
    "tide_height = tides.pressure.values\n",
    "bound_conc = np.full(len(tides.index), bound_conc)\n",
    "conc = np.zeros(len(tides.index))\n",
    "deposited_sediment = np.zeros(len(tides.index))\n",
    "depth = np.full(len(tides.index), np.nan)\n",
    "\n",
    "counter = np.arange(1, len(index))\n",
    "\n",
    "for t in tqdm(\n",
    "    counter,\n",
    "    total=len(index[1:]),\n",
    "    unit=\"steps\",\n",
    "):\n",
    "    elev[t] = aggrade(elev[t - 1], elev_change[t - 1], organic, compaction, subsidence)\n",
    "    conc[t] = calc_conc(\n",
    "        bound_conc[t],\n",
    "        tide_height[t],\n",
    "        tide_height[t - 1],\n",
    "        conc[t - 1],\n",
    "        elev[t],\n",
    "        elev[t - 1],\n",
    "        settle_rate,\n",
    "        timestep_sec\n",
    "    )\n",
    "    deposited_sediment[t] = accumulate_sediment(\n",
    "        conc[t], settle_rate, timestep_sec\n",
    "    )\n",
    "    elev_change[t] = deposited_sediment[t] / bulk_den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data={'elev': elev, 'tide': tide_height, 'bound_conc': bound_conc, 'conc': conc,'deposited_sediment': deposited_sediment, 'elev_change': elev_change}, index=index)\n",
    "data['depth'] = data.tide - data.elev\n",
    "data.depth = np.where(data.depth < 0, 0, data.depth)\n",
    "data['suspended_sediment'] = data.conc * data.depth\n",
    "data['incoming_sediment'] = bound_conc * (data.depth - data.depth.shift(1))\n",
    "data.incoming_sediment = np.where(data.incoming_sediment < 0, 0, data.incoming_sediment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.index >= '2014-05-18 01:05:31+06:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = data.copy()\n",
    "plot_data = plot_data.sample(frac=1)\n",
    "start = 0\n",
    "end = -1\n",
    "ticks = mpl.dates.YearLocator()\n",
    "\n",
    "figure, axes = plt.subplots(nrows=6, ncols=1, figsize=(10,15))\n",
    "p1 = plot_data[start:end][['tide']].plot(ax=axes[0]).xaxis.set_visible(False)\n",
    "plot_data[start:end][['elev']].plot(ax=axes[0], color='black', ls='--')\n",
    "p2 = plot_data[start:end][['conc']].plot(ax=axes[1]).xaxis.set_visible(False)\n",
    "p3 = plot_data[start:end][['suspended_sediment']].plot(ax=axes[2]).xaxis.set_visible(False)\n",
    "p4 = plot_data[start:end][['incoming_sediment']].plot(ax=axes[3]).xaxis.set_visible(False)\n",
    "p5 = plot_data[start:end][['deposited_sediment']].plot(ax=axes[4]).xaxis.set_visible(False)\n",
    "p6 = plot_data[start:end][['elev']].plot(ax=axes[5])\n",
    "axes[5].xaxis.set_major_locator(ticks)\n",
    "axes[5].xaxis.set_major_formatter(mpl.dates.DateFormatter('%Y'))\n",
    "axes[5].set_xlabel('Year')\n",
    "ylabels = ['Height (m)', 'Concentration ($kg \\cdot m^{3}$)', 'Suspended ($kg \\cdot m^{-2}$)', 'Incoming ($kg \\cdot m^{-2}$)', 'Deposited ($kg \\cdot m^{-2}$)', 'Elevation (m)']\n",
    "count = 0\n",
    "for ax in axes:\n",
    "    ax.margins(x=0)\n",
    "    ax.set_ylabel(ylabels[count])\n",
    "    ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "    ax.get_legend().remove()\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data.depth[data.depth>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.elev[-1] - data.elev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polder = pd.read_feather('for_jonathan/polder_20yr.feather').set_index('Datetime')\n",
    "natural = pd.read_feather('for_jonathan/natural_20yr.feather').set_index('Datetime')\n",
    "both = pd.concat([natural.elev, polder.elev], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = -1\n",
    "frac = 0.001\n",
    "p = both[start:end][['elev']].sample(frac=frac).plot(figsize=(8,5))\n",
    "p.margins(x=0)\n",
    "p.legend(['natural', 'polder'])\n",
    "p.set_xlabel('Year')\n",
    "p.set_ylabel('Aggradation (m)')\n",
    "p.savefig('../figures/model_figs/out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD MODEL [DEPRICATED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for running the model.\n",
    "# def run_model(tides, ssc, params, n = None)\n",
    "# Instead of grain_dia, you use params.grain_dia, params.grain_rho, etc.\n",
    "def run_model(\n",
    "    tides, ssc, grain_dia, grain_rho, bulk_rho, dP=0, dO=0, dM=0, A=1, z0=0, n=None\n",
    "):\n",
    "    global num_runs\n",
    "\n",
    "    # Function that sets the background SSC value given a method.\n",
    "    def find_ssc(ssc, method, timestamp=None):\n",
    "        if method == \"constant\":\n",
    "            return ssc\n",
    "        elif method == \"weekly\":\n",
    "            week = timestamp.week\n",
    "            ssc = ssc.loc[week].values[0]\n",
    "            return ssc\n",
    "\n",
    "    # Return suspended sediment values from a csv of average weekly suspended sediment for P32. When tide is below the\n",
    "    # the platform (no sedimentation) or the tide is falling (net export), \"0\" is returned.\n",
    "\n",
    "    # Calculate the concentration within the water column for a given timestep\n",
    "    # old method\n",
    "    #     def calc_c(c0, h, h_min_1, dh, c_min_1, z, ws, dt):\n",
    "    #         if (h > z and dh > 0):\n",
    "    #             return (-c_min_1 * ws * dt + (c0-c_min_1) * (h - h_min_1)) / (h_min_1-z) + c_min_1\n",
    "    #         elif (h > z and dh < 0):\n",
    "    #             return (-c_min_1 * ws * dt) / (h_min_1-z) + c_min_1\n",
    "    #         else:\n",
    "    #             return 0\n",
    "\n",
    "    # Method from work with David\n",
    "    def calc_c(c0, h, h_min_1, dh, c_min_1, z, ws, dt):\n",
    "        if h > z and dh > 0:\n",
    "            return (-c_min_1 * ws * dt + (c0 - c_min_1) * (h - h_min_1)) / (\n",
    "                h_min_1 - z\n",
    "            ) + c_min_1\n",
    "        elif h > z and dh < 0:\n",
    "            return (-c_min_1 * ws * dt) / (h_min_1 - z) + c_min_1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    #     added by JG\n",
    "    #     global g_calc_c\n",
    "    #     g_calc_c = calc_c\n",
    "\n",
    "    # Calculate the change in elevation for a given timestep\n",
    "    def calc_dz(c, ws, rho, dt):\n",
    "        return ws * c * dt / rho\n",
    "\n",
    "    # Add the change of elevation back to the original elevation\n",
    "    def calc_z(z_min_1, dz_min_1, dO, dP, dM):\n",
    "        return z_min_1 + dz_min_1 + dO - dP - dM\n",
    "\n",
    "    # Set method to be used for ssc based on ssc input type\n",
    "    if isinstance(ssc, float):\n",
    "        ssc_method = \"constant\"\n",
    "    elif isinstance(ssc, pd.DataFrame):\n",
    "        ssc_method = \"weekly\"\n",
    "\n",
    "    # Set Datetime as the index. Feather does not export non-integer indices.\n",
    "    tides = tides.set_index(\"Datetime\")\n",
    "    index = tides.index\n",
    "    dt = index[1] - index[0]\n",
    "    dt_sec = dt.total_seconds()\n",
    "\n",
    "    # Convert constant rates from yearly to dt\n",
    "    dO = dO / 8760 / 60 / 60 * dt_sec\n",
    "    dP = dP / 8760 / 60 / 60 * dt_sec\n",
    "    dM = dM / 8760 / 60 / 60 * dt_sec\n",
    "\n",
    "    # Assume density and viscosity of water\n",
    "    fluid_rho = 1000\n",
    "    fluid_visc = 0.001\n",
    "    g = 9.8\n",
    "\n",
    "    # Calculate settling velocity using Stokes settling. Considered an upper bound for possible settling velocity.\n",
    "    ws = (2 / 9 * (grain_rho - fluid_rho) / fluid_visc) * g * (grain_dia / 2) ** 2\n",
    "\n",
    "    # Initialize numpy arrays for efficiency\n",
    "    z = np.zeros(len(tides.index))\n",
    "    z[0] = z0\n",
    "    h = tides.pressure.values\n",
    "    dh = np.insert(np.diff(h) / dt_sec, 0, np.nan)\n",
    "    inundated = np.zeros(len(tides.index))\n",
    "    inundation_depth = np.zeros(len(tides.index))\n",
    "    C0 = np.zeros(len(tides.index))\n",
    "    C = np.zeros(len(tides.index))\n",
    "    dz = np.zeros(len(tides.index))\n",
    "    SSC = np.zeros(len(tides.index))\n",
    "\n",
    "    #     def update_elevation(z, SSC, C0, C, dz, h, dh, dO, dP, dM, A, ssc, bulk_rho, dt_sec, ws, ssc_method, timestamp):\n",
    "    #         z[t] = calc_z(z[t-1], dz[t-1], dO, dP, dM)\n",
    "    #         SSC[t] = find_ssc(ssc, method=ssc_method, timestamp=index[t])\n",
    "    #         C0[t] = calc_c0(h[t], dh[t], ssc, z[t], A)\n",
    "    #         C[t] = calc_c(C0[t], h[t], h[t-1], dh[t], C[t-1], z[t], ws, dt_sec)\n",
    "    #         dz[t] = calc_dz(C[t], ws, bulk_rho, dt_sec)\n",
    "\n",
    "    # For loop to calculate backwards difference approximation.\n",
    "    # TQDM is a wrapper that shows a status bar while calculating.\n",
    "    counter = np.arange(1, len(index))\n",
    "    for t in tqdm(\n",
    "        counter,\n",
    "        desc=\"Run {0} of {1} [PID: {2}]\".format(n, num_runs, os.getpid()),\n",
    "        total=len(index[1:]),\n",
    "        unit=\"steps\",\n",
    "    ):\n",
    "        # update_elevation(z = z, SSC = SSC, C0 = C0, C = C,\n",
    "        #                  dz = dz, h =  h, dh = dh, dO = dO, dP = dP, dM = dM,\n",
    "        #                  A = A, ssc = ssc, bulk_rho = bulk_rho, dt_sec = dt_sec,\n",
    "        #                  ws = ws, ssc_method = ssc_method, timestamp = timestamp)\n",
    "        z[t] = calc_z(z[t - 1], dz[t - 1], dO, dP, dM)\n",
    "        SSC[t] = find_ssc(ssc, method=ssc_method, timestamp=index[t])\n",
    "        C0[t] = SSC[t] * A\n",
    "        C[t] = calc_c(C0[t], h[t], h[t - 1], dh[t], C[t - 1], z[t], ws, dt_sec)\n",
    "        dz[t] = calc_dz(C[t], ws, bulk_rho, dt_sec)\n",
    "        # Flag if inundated and by how much\n",
    "        if h[t] - z[t] >= 0:\n",
    "            inundated[t] = 1\n",
    "            inundation_depth[t] = h[t] - z[t]\n",
    "\n",
    "    # Create pandas dataframe from numpy arrays of finite difference results\n",
    "    d = {\n",
    "        \"h\": h,\n",
    "        \"dh\": dh,\n",
    "        \"C0\": C0,\n",
    "        \"C\": C,\n",
    "        \"dz\": dz,\n",
    "        \"z\": z,\n",
    "        \"inundated\": inundated,\n",
    "        \"inundation_depth\": inundation_depth,\n",
    "    }\n",
    "    df = pd.DataFrame(data=d, index=tides.index)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create helper functions for parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to be called by parallel function (e.g. imap_unordered). This is necessary because imap only accepts\n",
    "# one function and one iterable. Using this parser allows the model to be run with multiple iterables package as\n",
    "# one tuple\n",
    "def parallel_parser(in_data):\n",
    "\n",
    "    n = in_data[\"n\"]  # number to explicitly set print line for TQDM. Not working.\n",
    "\n",
    "    # Load tides for a given run_length, dt, and slr from the tide library.\n",
    "    run_length = in_data[\"run_length\"]\n",
    "    dt = in_data[\"dt\"]\n",
    "    slr = in_data[\"slr\"]\n",
    "    tides = feather.read_dataframe(\n",
    "        \"./data/interim/tides/tides-yr_{0}-dt_{1}-slr_{2}.feather\".format(\n",
    "            run_length, int(pd.to_timedelta(dt).total_seconds() / 60 / 60), \"%.4f\" % slr\n",
    "        )\n",
    "    )\n",
    "    # Set Datetime as the index. Feather does not export non-integer indices.\n",
    "    tides = tides.set_index(\"Datetime\")\n",
    "\n",
    "    # Load weekly ssc data. Original data from OBS sensor deployed at Sutarkhali. Data is in 1-min increments. Developed\n",
    "    # a model to predict SSC by week of the year (incoming_ssc.R). Output of this script is the weekly SSC loaded here.\n",
    "    ssc_factor = in_data[\"ssc_factor\"]  # scaling factor used to adjust SSC\n",
    "    ssc_file = \"./data/processed/ssc_by_week.csv\"\n",
    "    ssc = pd.read_csv(ssc_file, index_col=0) * ssc_factor\n",
    "\n",
    "    # set parameters for model run\n",
    "    grain_dia = in_data[\"grain_dia\"]\n",
    "    grain_rho = in_data[\"grain_rho\"]\n",
    "    bulk_rho = in_data[\"bulk_rho\"]\n",
    "    dP = in_data[\"dP\"]\n",
    "    dO = in_data[\"dO\"]\n",
    "    dM = in_data[\"dM\"]\n",
    "    A = in_data[\"A\"]\n",
    "    z0 = in_data[\"z0\"]\n",
    "\n",
    "    # run model\n",
    "    # params = in_data[\"params\"]\n",
    "    # run_model(tides = tides, ssc = ssc, params = params, n = n)\n",
    "    run_model(tides, ssc, grain_dia, grain_rho, bulk_rho, dP, dO, dM, A, z0, n=n)\n",
    "\n",
    "    # Write results to a feather file\n",
    "    out_name = \"yr_{0}-slr_{1}-grain_dia_{2}-grain_rho_{3}-bulk_rho_{4}-sscfactor_{5}-dP_{6}-dM_{7}-A_{8}-z0_{9}.feather\".format(\n",
    "        run_length, slr, grain_dia, grain_rho, bulk_rho, ssc_factor, dP, dM, A, z0\n",
    "    )\n",
    "    feather.write_dataframe(\n",
    "        df.reset_index(), \"./data/interim/results/{0}\".format(out_name)\n",
    "    )\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parallel method\n",
    "if parallel == True:\n",
    "\n",
    "    # Make combos of parameters\n",
    "    model_runs = make_combos(\n",
    "        run_length,\n",
    "        dt,\n",
    "        slr,\n",
    "        ssc_factor,\n",
    "        grain_dia,\n",
    "        grain_rho,\n",
    "        bulk_rho,\n",
    "        dP,\n",
    "        dO,\n",
    "        dM,\n",
    "        A,\n",
    "        z0,\n",
    "    )\n",
    "\n",
    "    # Count number of models to be run\n",
    "    num_runs = len(model_runs)\n",
    "\n",
    "    # Initialize pool and run models\n",
    "    with mp.Pool(poolsize) as pool:\n",
    "        for result in pool.imap_unordered(\n",
    "            parallel_parser, model_runs, chunksize=chunksize\n",
    "        ):\n",
    "            pass\n",
    "\n",
    "# Single core method\n",
    "elif parallel == False:\n",
    "\n",
    "    # Load tides from tide library\n",
    "    tides = feather.read_dataframe(\n",
    "        \"./data/interim/tides/tides-yr_{0}-dt_{1}-slr_{2}.feather\".format(\n",
    "            run_length, int(pd.to_timedelta(dt).total_seconds() / 60), \"%.4f\" % slr\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Set number of models to be run\n",
    "    num_runs = 1\n",
    "\n",
    "    # run model\n",
    "    # df = run_model(tides = tides, ssc = ssc, params = params, n = 1)\n",
    "    df = run_model(tides, ssc, grain_dia, grain_rho, bulk_rho, dP, dO, dM, A, z0, n=1)\n",
    "\n",
    "    # write results to feather file\n",
    "    out_name = \"yr_{0}-slr_{1}-grain_dia_{2}-grain_rho_{3}-bulk_rho_{4}-sscfactor_{5}-dP_{6}-dM_{7}-A_{8}-z0_{9}.feather\".format(\n",
    "        run_length, slr, grain_dia, grain_rho, bulk_rho, ssc_factor, dP, dM, A, z0\n",
    "    )\n",
    "    feather.write_dataframe(\n",
    "        df.reset_index(), \"./data/interim/results/{0}\".format(out_name)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
